{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teSVc4i7wTRh"
      },
      "source": [
        "# Boston housing price regression dataset\n",
        "Dataset taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
        "\n",
        "Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Targets are the median values of the houses at a location (in k$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0-16cYlHzzP"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLz1Ckvfvn6D"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CWrzVTLOvn6M",
        "outputId": "c136879c-cf35-4dcd-a188-b6ec302627b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.0-rc2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tr_z_2YHxsz5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "# boston_housing.load_data() function returns 2 tuples, one for train data and \n",
        "# other for test data. We will take only train data here.\n",
        "(features, actual_prices), _ = boston_housing.load_data(test_split=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "WTYuZMy-xsz8",
        "outputId": "fad0183f-8cd7-46a1-dc0f-db8600cc949c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples:  506\n",
            "Number of features for each example:  13\n",
            "Shape of actual prices data:  (506,)\n"
          ]
        }
      ],
      "source": [
        "print('Number of examples: ', features.shape[0])\n",
        "print('Number of features for each example: ', features.shape[1])\n",
        "print('Shape of actual prices data: ', actual_prices.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfY8Km1Zzyt2"
      },
      "source": [
        "### Convert data type\n",
        "- Convert \"features\" variable to \"float32\"\n",
        "- Convert \"actual_prices\" variable to \"float32\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko7nnQVbYENh"
      },
      "outputs": [],
      "source": [
        "features = tf.dtypes.cast(features, tf.float32)\n",
        "actual_prices = tf.dtypes.cast(actual_prices, tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0Tfe00X78wB"
      },
      "outputs": [],
      "source": [
        "features = tf.math.l2_normalize(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o9RPWVTxs0O"
      },
      "outputs": [],
      "source": [
        "W = tf.zeros(shape=(13, 1))\n",
        "b = tf.zeros(shape=(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMXXYdOSxs0Q"
      },
      "source": [
        "### Get prediction\n",
        "- Define a function to get prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8Cty1y0xs0S"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def prediction(features, W, b):\n",
        "    y_pred = tf.add(tf.matmul(features, W), b)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmS3Tauxs0V"
      },
      "source": [
        "### Calculate loss\n",
        "- Calculate loss using predictions\n",
        "- Define a function to calculate loss\n",
        "- We are calculating mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FRXmDd5xs0X"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss(y_actual, y_predicted):\n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    return avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkOzAUUsTmF_"
      },
      "source": [
        "### Function to train the Model\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R4uieGYLYtM"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train(x, y_actual, W, b, learning_rate=0.01):\n",
        "    \n",
        "    # Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        t.watch([W,b])\n",
        "        current_prediction = prediction(x, W, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    # Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[W, b])\n",
        "    \n",
        "    # Update Weights and Bias\n",
        "    w = W - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "    \n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeN0deOvT81N"
      },
      "source": [
        "### Train the model for 100 epochs \n",
        "- Observe the training loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jjkn4gUgLevE",
        "outputId": "368fce1a-c090-4219-8dc0-21c3bee220dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Training Loss on iteration 0 572.0403\n",
            "Current Training Loss on iteration 1 552.73114\n",
            "Current Training Loss on iteration 2 534.1873\n",
            "Current Training Loss on iteration 3 516.3784\n",
            "Current Training Loss on iteration 4 499.27536\n",
            "Current Training Loss on iteration 5 482.8501\n",
            "Current Training Loss on iteration 6 467.07574\n",
            "Current Training Loss on iteration 7 451.92657\n",
            "Current Training Loss on iteration 8 437.3777\n",
            "Current Training Loss on iteration 9 423.4054\n",
            "Current Training Loss on iteration 10 409.98672\n",
            "Current Training Loss on iteration 11 397.09985\n",
            "Current Training Loss on iteration 12 384.7236\n",
            "Current Training Loss on iteration 13 372.8377\n",
            "Current Training Loss on iteration 14 361.42282\n",
            "Current Training Loss on iteration 15 350.4602\n",
            "Current Training Loss on iteration 16 339.932\n",
            "Current Training Loss on iteration 17 329.82086\n",
            "Current Training Loss on iteration 18 320.11035\n",
            "Current Training Loss on iteration 19 310.78455\n",
            "Current Training Loss on iteration 20 301.82825\n",
            "Current Training Loss on iteration 21 293.22678\n",
            "Current Training Loss on iteration 22 284.966\n",
            "Current Training Loss on iteration 23 277.03247\n",
            "Current Training Loss on iteration 24 269.41324\n",
            "Current Training Loss on iteration 25 262.09583\n",
            "Current Training Loss on iteration 26 255.06827\n",
            "Current Training Loss on iteration 27 248.31905\n",
            "Current Training Loss on iteration 28 241.83719\n",
            "Current Training Loss on iteration 29 235.61206\n",
            "Current Training Loss on iteration 30 229.63345\n",
            "Current Training Loss on iteration 31 223.89166\n",
            "Current Training Loss on iteration 32 218.37726\n",
            "Current Training Loss on iteration 33 213.08125\n",
            "Current Training Loss on iteration 34 207.99496\n",
            "Current Training Loss on iteration 35 203.11008\n",
            "Current Training Loss on iteration 36 198.41862\n",
            "Current Training Loss on iteration 37 193.91298\n",
            "Current Training Loss on iteration 38 189.58571\n",
            "Current Training Loss on iteration 39 185.42976\n",
            "Current Training Loss on iteration 40 181.43839\n",
            "Current Training Loss on iteration 41 177.60501\n",
            "Current Training Loss on iteration 42 173.9234\n",
            "Current Training Loss on iteration 43 170.38754\n",
            "Current Training Loss on iteration 44 166.99164\n",
            "Current Training Loss on iteration 45 163.73016\n",
            "Current Training Loss on iteration 46 160.5978\n",
            "Current Training Loss on iteration 47 157.5894\n",
            "Current Training Loss on iteration 48 154.70007\n",
            "Current Training Loss on iteration 49 151.92511\n",
            "Current Training Loss on iteration 50 149.25995\n",
            "Current Training Loss on iteration 51 146.70029\n",
            "Current Training Loss on iteration 52 144.2419\n",
            "Current Training Loss on iteration 53 141.8808\n",
            "Current Training Loss on iteration 54 139.61311\n",
            "Current Training Loss on iteration 55 137.43513\n",
            "Current Training Loss on iteration 56 135.34337\n",
            "Current Training Loss on iteration 57 133.33434\n",
            "Current Training Loss on iteration 58 131.40475\n",
            "Current Training Loss on iteration 59 129.55151\n",
            "Current Training Loss on iteration 60 127.77158\n",
            "Current Training Loss on iteration 61 126.06206\n",
            "Current Training Loss on iteration 62 124.420135\n",
            "Current Training Loss on iteration 63 122.84315\n",
            "Current Training Loss on iteration 64 121.32852\n",
            "Current Training Loss on iteration 65 119.87378\n",
            "Current Training Loss on iteration 66 118.47657\n",
            "Current Training Loss on iteration 67 117.1346\n",
            "Current Training Loss on iteration 68 115.84568\n",
            "Current Training Loss on iteration 69 114.60773\n",
            "Current Training Loss on iteration 70 113.4187\n",
            "Current Training Loss on iteration 71 112.276665\n",
            "Current Training Loss on iteration 72 111.17978\n",
            "Current Training Loss on iteration 73 110.12625\n",
            "Current Training Loss on iteration 74 109.11433\n",
            "Current Training Loss on iteration 75 108.14242\n",
            "Current Training Loss on iteration 76 107.20889\n",
            "Current Training Loss on iteration 77 106.312256\n",
            "Current Training Loss on iteration 78 105.451035\n",
            "Current Training Loss on iteration 79 104.62385\n",
            "Current Training Loss on iteration 80 103.82933\n",
            "Current Training Loss on iteration 81 103.06618\n",
            "Current Training Loss on iteration 82 102.333176\n",
            "Current Training Loss on iteration 83 101.62911\n",
            "Current Training Loss on iteration 84 100.95286\n",
            "Current Training Loss on iteration 85 100.30329\n",
            "Current Training Loss on iteration 86 99.67936\n",
            "Current Training Loss on iteration 87 99.08008\n",
            "Current Training Loss on iteration 88 98.50445\n",
            "Current Training Loss on iteration 89 97.95152\n",
            "Current Training Loss on iteration 90 97.420425\n",
            "Current Training Loss on iteration 91 96.91027\n",
            "Current Training Loss on iteration 92 96.42025\n",
            "Current Training Loss on iteration 93 95.94956\n",
            "Current Training Loss on iteration 94 95.49743\n",
            "Current Training Loss on iteration 95 95.06313\n",
            "Current Training Loss on iteration 96 94.64596\n",
            "Current Training Loss on iteration 97 94.24525\n",
            "Current Training Loss on iteration 98 93.86032\n",
            "Current Training Loss on iteration 99 93.49058\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):    \n",
        "    W, b = train(features, actual_prices, W, b)\n",
        "    print('Current Training Loss on iteration', i, loss(actual_prices, prediction(features, W, b)).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanvD93FV0_k"
      },
      "source": [
        "### Observing values of Weight\n",
        "- Let's see what are the updated values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "QSqpy4gtWaOD",
        "outputId": "994518f9-ff66-4b74-c542-42f976942bf9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.3930967e-03],\n",
              "       [1.6973170e-02],\n",
              "       [1.6626865e-02],\n",
              "       [1.0328969e-04],\n",
              "       [8.2824891e-04],\n",
              "       [9.3846284e-03],\n",
              "       [1.0239015e-01],\n",
              "       [5.6676129e-03],\n",
              "       [1.4254721e-02],\n",
              "       [6.0949117e-01],\n",
              "       [2.7557964e-02],\n",
              "       [5.3259981e-01],\n",
              "       [1.8891934e-02]], dtype=float32)"
            ]
          },
          "execution_count": 62,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "W.numpy()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}